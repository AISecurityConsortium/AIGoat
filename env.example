# Red Team Shop - Environment Configuration
# This file contains all configurable environment variables used by the application
# Copy this file to .env and update the values as needed

# =============================================================================
# BACKEND CONFIGURATION (Django)
# =============================================================================

# Django Security Settings
SECRET_KEY=django-insecure-ezr0q%1q61hl92z5=_5g0ef!-afexujpshdbey3kyp*ud
DEBUG=True
ALLOWED_HOSTS=localhost,127.0.0.1

# AI Service Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral
OLLAMA_MAX_INPUT_LENGTH=1000   # Maximum input length for validation (shared)

# =============================================================================
# LLM PARAMETERS - SEPARATE CONFIGURATION FOR EACH ASSISTANT
# =============================================================================

# CRACKY AI ASSISTANT - Creative and unpredictable behavior
CRACKY_TEMPERATURE=0.7          # High creativity for "cracky" responses
CRACKY_TOP_P=0.9               # High diversity in word choices
CRACKY_TOP_K=40                # Balanced diversity
CRACKY_NUM_PREDICT=500         # Longer responses for creativity
CRACKY_TIMEOUT=60              # Standard timeout

# SEARCH AI ASSISTANT - Focused and relevant search results
SEARCH_TEMPERATURE=0.5          # Moderate creativity for search relevance
SEARCH_TOP_P=0.8               # Focused vocabulary
SEARCH_TOP_K=30                 # More focused responses
SEARCH_NUM_PREDICT=300         # Shorter, focused responses
SEARCH_TIMEOUT=45              # Faster timeout for search

# RAG CHAT ASSISTANT - Factual and knowledge-based responses
RAG_TEMPERATURE=0.3             # Low creativity for factual accuracy
RAG_TOP_P=0.8                  # Consistent vocabulary
RAG_TOP_K=40                   # Balanced for knowledge retrieval
RAG_NUM_PREDICT=500           # Longer responses for detailed answers
RAG_TIMEOUT=60                # Standard timeout

# =============================================================================
# FRONTEND CONFIGURATION (React)
# =============================================================================

# API Configuration
REACT_APP_API_URL=http://localhost:8000

# =============================================================================
# DEPLOYMENT NOTES
# =============================================================================
# 
# For production deployment:
# 1. Set SECRET_KEY to a secure random string
# 2. Set DEBUG=False
# 3. Update ALLOWED_HOSTS with your domain/IP
# 4. Update OLLAMA_BASE_URL to your Ollama service URL
# 5. Update REACT_APP_API_URL to your backend API URL
#
# Example production values:
# SECRET_KEY=your-super-secret-key-here
# DEBUG=False
# ALLOWED_HOSTS=your-domain.com,your-ip-address
# OLLAMA_BASE_URL=https://your-ollama-service.com
# OLLAMA_MODEL=llama2  # or mistral, codellama, etc.
# OLLAMA_TEMPERATURE=0.3  # Lower for more consistent responses
# OLLAMA_TOP_P=0.8        # Adjust for response diversity
# OLLAMA_TOP_K=20         # Lower for faster generation
# OLLAMA_NUM_PREDICT=300  # Adjust based on model capabilities
# OLLAMA_MAX_INPUT_LENGTH=2000  # Increase for longer inputs
# OLLAMA_TIMEOUT=120      # Increase for slower models
# REACT_APP_API_URL=https://your-backend-api.com
